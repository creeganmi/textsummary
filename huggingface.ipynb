{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Tutorial on HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment pipeline performs text classification on input and determines if it is positive or negative and outputs the given confidence interval using 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9550794363021851}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I'm watching an hour long Natural Language Processing Video to understand this process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pass multiple statements together for pipeline to process together as a patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998338222503662},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997290968894958}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier([\"I am very excited for New Years Eve!\", \n",
    "\"I hate not being able to travel as much due to COVID-19.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero text classification pipeline for more general text classification, in this case taking the input text is related to the labels education, business, or programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This course is about learning to explore and visualizing real-world datasets using Python',\n",
       " 'labels': ['programming', 'education', 'business'],\n",
       " 'scores': [0.66228187084198, 0.32246798276901245, 0.015250138007104397]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\"This course is about learning to explore and visualizing real-world datasets using Python\", \n",
    "candidate_labels=[\"education\",\"business\",\"programming\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generation pipeline auto-completes a given prompt and the output is generated with a bit of randomness so it changes each time\n",
    "### the text I am using in this example is from the APAN website for Python class description \n",
    "### the sentence is: The students in this course will learn to examine raw data with the purpose of deriving insights and drawing conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The students in this course will learn to examine raw data with the help of a database of hundreds of years of human data, making the process of applying basic statistical techniques in an interactive manner possible. The students will also use their intuition and insight to create'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"The students in this course will learn to examine raw data with the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when a model is not explicitely provided, the default model for each associated task is selected. However, you can select any model that has been pretrained for a task on https://huggingface.co/models\n",
    "### lets do text generation but with another model, jpg2 and see what happens (this is a lighter version for the gpd2 model)\n",
    "### we can specify several arguments like max length of generated text and number of sentences we want to return bc there is some randomness in the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The students in this course will learn to examine raw data with the <mask> of deriving insights and drawing conclusions based on their knowledge, including,'},\n",
       " {'generated_text': 'The students in this course will learn to examine raw data with the <mask> of deriving insights and drawing conclusions which have previously been published in the'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"The students in this course will learn to examine raw data with the <mask> of deriving insights and drawing conclusions\", \n",
    "    max_length=30, \n",
    "    num_return_sequences=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the fill-mask pipeline had a pretraining objective to guess the value of missing words in a sentence.\n",
    "### in this case, we ask for the 2 most likely values ofr missing words according to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'The students in this course will learn to examine raw data with the purpose of deriving hypotheses and drawing conclusions',\n",
       "  'score': 0.2825345993041992,\n",
       "  'token': 44850,\n",
       "  'token_str': ' hypotheses'},\n",
       " {'sequence': 'The students in this course will learn to examine raw data with the purpose of deriving trends and drawing conclusions',\n",
       "  'score': 0.08611495047807693,\n",
       "  'token': 3926,\n",
       "  'token_str': ' trends'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"The students in this course will learn to examine raw data with the purpose of deriving <mask> and drawing conclusions\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the NER pipelines classifies each word in a sentence instead of the sentence as wall. For example, using named entity recognition such as persons, organizations, or locations in a sentence\n",
    "### the group pipeline entity use is to make the pipeline group together different words linked to the same entity such as New York and Goldman Sachs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "C:\\Users\\creeg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\pipelines\\token_classification.py:128: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99904186,\n",
       "  'word': 'Michael',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.99903166,\n",
       "  'word': 'Goldman Sachs',\n",
       "  'start': 33,\n",
       "  'end': 46},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99840856,\n",
       "  'word': 'Manhattan',\n",
       "  'start': 50,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Michael and I work at Goldman Sachs in Manhattan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another task avaialble with a pipeline api is extractive question answering providing a context and a question the mdoel will identify the span of text containing the answer to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.7812808752059937,\n",
       " 'start': 33,\n",
       " 'end': 46,\n",
       " 'answer': 'Goldman Sachs'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\", \n",
    "    context=\"My name is Michael and I work at Goldman Sachs in Manhattan.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting summaries of long text is also a task that transformers libraries can help with using the summarization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Spotify has removed the work of hundreds of comedians from its platform . Comedians include Robin Williams, Tiffany Haddish, Kevin Hart and Robin Williams . Spoken Giants, which represents some of the affected comedians, says it never requested the content\\'s removal . A Spotify spokesperson says the streaming platform had already paid \"significant amounts of money\" to offer the comedy content to listeners .'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"Spotify is not joking around amid a dispute over royalties for comedy content. The streaming giant has removed the work of hundreds of comedians from its platform -- including Tiffany Haddish, Kevin Hart and the late Robin Williams -- according to rights agency Spoken Giants.\n",
    "\n",
    "Spoken Giants, which represents some of the affected comedians, describes itself as \"the first global rights administration company for the owners and creators of spoken word copyrights,\" and aims to get streaming platforms to pay comedians for writing jokes in the same way songwriters are paid.\n",
    "The group told CNN that the take-down happened on November 24, and said that it never requested the content's removal.\n",
    "\"Unfortunately, Spotify removed the work of individual comedians rather than continue to negotiate,\" CEO Jim King told CNN.\n",
    "\"With this take-down, individual comedians are now being penalized for collectively requesting the same compensation songwriters receive,\" he added. \"After Spotify removed our members' work, we reached out but have not received a response. We have now requested an immediate meeting to resolve this situation.\"\n",
    "A Spotify spokesperson told CNN that the streaming platform had already paid \"significant amounts of money\" to offer the comedy content to listeners, and \"would love to continue to do so.\"\n",
    "\"However, given that Spoken Giants is disputing what rights various licensors have, it's imperative that the labels that distribute this content, Spotify and Spoken Giants come together to resolve this issue to ensure this content remains available to fans around the globe,\" the spokesperson said.\n",
    "Although the content is still available on other platforms including Pandora and Sirius, Spoken Giants said comedians with lower profiles and revenues could suffer from losing Spotify as a platform.\n",
    "On social media, New York-based comedian Joe Zimmerman called the move \"corporate bullying.\" Another New York-based comedian, Liz Miele, tweeted that her albums had also been removed from the platform because comedians \"had the audacity to ask for money owed to us,\" and jokingly compared herself to singer Taylor Swift.\n",
    "Swift was previously engaged in a dispute with Spotify, arguing artists were not paid enough. The singer pulled her entire catalog from the platform in 2014, but reversed her decision in 2017\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I would translate this more as there is a little truth behind every joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Between jokes and jokes the truth comes out.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "translator(\"Entre broma y broma la verdad se asoma.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### russian translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'За каждой шуткой есть немного правды.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ru\")\n",
    "translator(\"There is a little truth behind every joke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test out en zh translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '每一个笑话背后都有一点点真相。'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "translator(\"There is a little truth behind every joke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoders, Encoders, and Transformers\n",
    "####  decoders are trained to guess next word which is why they are good at generating text,\n",
    "#### encoder models are pre-trained by filling random mask in a sentence so better at output,\n",
    "#### transformers models are just massive,\n",
    "#### lets build summarization model and assess accuracy using zero shot on both the article and summary, bleu, and a custom scoring methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "#### transfer learning is when take pre-trained model on a lot of data and reuse that model to fine-tune it on new task you want to work with - so by reusing the model instead of doing one from scratch means you needs less data to train your model\n",
    "#### gpd2 was pre-trained using the content of 45 million links posted by users on reddit - good model for guessing next word in sentence\n",
    "#### predict value of randmoly masked words - bert was retrained this way using english wikipedia and 11,000 unpublished books\n",
    "#### but pre-trained model contains knowledge but also bias it already has, ex: imagenet contains images from united states and western europe so models fine-tuned on it usually do better on images from these countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "#### decoders, encoders, sequence to sequence (encoder-decoder)\n",
    "#### encoder converts to numerical - is bi-directional, self-attention (inputs)\n",
    "#### decoder \"decodes\" the representations from the encoder (output probabilities), uni-directional, auto-regressive  traditional use, masked self-attention\n",
    "#### encoder-decoder (sequence to sequence) combines decoder and encoder, the encoder accepts inputs and computes a high level representation of those inputs, these outputs are then passed to decoder that uses these outputs alongside other inputs to generate a prediction output which it will use in future - hence auto-regressive\n",
    "\n",
    "#### encoders - the attention mechanism is allowed to look at every word in the sentence so the word before and after, like BERT model, when need to guess value of masked word it is useful to look at what was before and after\n",
    "#### decoder - like gpt have to predict the next word so if they were allowed to look at word after, it would be cheating so theyt are only allowed to look at what was before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Models\n",
    "\n",
    "#### BERT is popular encoder model, ALBERT, ELECTRA, RoBERTa, DistilBERT\n",
    "#### retreives numerical representation of each word\n",
    "#### encoder outputs one sequence of numbers per input word aka feature vector/tensor\n",
    "#### each vector is numerical representation of word and the dimension of that vector is defined by the architecture of the model - for base BERT model it is 768\n",
    "#### these vectors contain the value of a word but contextualized, for example the vector representation of the word \"to\" isnt just \"to\", it also takes iinto account the words around it which are called context (right and left context)\n",
    "#### \"Welcome to NYC\" - for the word \"to\" the left context is \"Welcome\" and the right context is \"NYC\" -- output is based on contexts so it is a contextualized vector thanks to self-attention mechanism\n",
    "#### self attention mechanism relates to different positions of different words in a single sequence in order to calculate\n",
    "#### should use encoder as stand alone models for sequence classification, question answering, and masked language modeling, etc... very powerful at extracting vectors that are meaningful in a sequence \n",
    "#### encoders really shine in mask langauge modeling MLM predict hidding word in sequence of words -- BERT was trained on this bc bi-directional inforamtion is crucial in this task\n",
    "#### requires semantic understanding of as well as syntactic understanding\n",
    "#### encoders are good at sequence classifrication like sentiment-analysis - aim is to identify sentiment of a sequence like 1-5 stars for review, positive or negative rating of a sequence - even if words are same sequence can make something mean something completely different\n",
    "#### BERT has maximum length of 512 words so generally cannot use input of greater than 512 words in model, some models like long former can accept longer context so should read documentation for specific encoders and their abilities, can split sentence into several parts of 512 words and pass each of those chunks into model and can average what you get at the end to try to train a classifier for larger sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Models\n",
    "\n",
    "#### popular decoder model is gpt2\n",
    "#### can use generally for same tasks as decoders but with little loss on performance\n",
    "#### \"Welcome to NYC\" -- gets numerical representation for each word, outputs one sequence of numbers per input word, this is a feature vector/tensor\n",
    "#### one vector per word passed through decoder, each vector is numerical represntation of word, dimension of vector defined by the architecture of the model\n",
    "#### differs from encoder is with self attention mechanism - uses masked self attention\n",
    "#### wrod \"to\" would be unmodified by NYC word bc right context of word will all be masked, doesnt benefit from bi-directional context, only single context depending on what is masked\n",
    "#### self-attention mechanism- provides additional mask to hide context in one direction, so vector is not affected by words in the hidden context\n",
    "#### should use as standalone models generate numerical, and can be used in wide variety of task, but having only access to left context would be great at text generation / ability to generate word or sequence of words given a string of words aka causal langauge modeling or natual langauge generation\n",
    "#### causal language modeling -- start with word \"my\" outputs vector of numbers which could be single word that maps to all words known by model (language modeling head) - and will predict most probabile following words, adds that to sequence so if picks \"name\" then uses \"my name\" to predict next word aka auto-regressive\n",
    "#### passes \"my name\" through decoder and then says \"is\" -- gpt2 has maximum context side of 1024 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Models (combine encoders and decoders) / Encoder-Decoder Architecture\n",
    "\n",
    "#### popular encoder-decode model is t5, BART, ProphetNet, mT5, M2M100, Pegasus, MarianMT, mBART, etc...\n",
    "#### encoder takes words as inputs passes through encoder, and gives numerical representation for each word passed through, and contains info about meaning of sequence\n",
    "#### decoder is passed outputs from encoder directly, in addition given a sequence sequence, when prompting decoder with no initial sequence, we can give it the value that indicates the start of a sequence (this is where magic happens)\n",
    "#### encoder accepts sequence as input and computes a prediction and outputs a numerical representation, then sends that over to decode, it has in a sense  \"encodede\" that sequence and the decoder uses this input alongside its usual sequence input will try to decode the sequence\n",
    "#### the decode decodes the sequence and outputs the word, dont need to make sense of it, but decode is decoding what encoder has output, start of sequence word indicates sohuld start decoding sequence - so dont need encoder anymore\n",
    "#### decode acts in auto-regressive manner so can take what was just output as an input - this in combination with numerical representation provided by encoder can be used to generate a second word and continues on and on untild ecode outputs value that is a stopping value like a . indicating end of a sequence\n",
    "#### so we ahve initial sequence sent to encoder, encoder output sent to decoder to be decoder, then can discard encoder after single use - then decoder can be used several times until we have generated every word that we need\n",
    "#### transduction (act of translating a sequence) -- can use transformer model built for that task -- take \"welcome to nyc\" translate to french - encoder translates words and pases welcome to bienvenue as start of sequence word to putput first word bienvenue and then use bienvenue is used as input sequence for deecode, this along with decoder numerical representation to predict second word \"to\" then uses bienvenue a and then can predict \"nyc\"\n",
    "#### encoder and decode often do not share weights - so very good - entire block encoder and be trained to understand sequence and extract relevant information, for translation scenario would mean parsing and understanding what was said in english, and extracting info from that language and put in vector dense information, then have decoder whose sole purpose is to decode numerical representation output by encoder, this decoder can be specialized in completely different lagnauge or modality like images\n",
    "#### encoder-decoder models are able to manage sequence to sequence tasks like translation, weights between encoder and decoder are not necessarily shared\n",
    "#### translate \"transformers are power\" in french takes 3 english words and outputs 4 french words les transformers sont puissants -- decoder can do this due to auto-regression standalone\n",
    "#### summarization very strong - since encoder and decoder are seperate can have very long context for encoder to handle text and smaller context for decoder which handles summarize sequence\n",
    "#### can also load an encoder and decoder inside an encoder-decoder model, so can choose to use specific encoders and decoders which are good at specific tasks - customizable\n",
    "#### cool to paraphrase from formal to informal using encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Limitations\n",
    "\n",
    "#### powerful but dont control input to output, more controlled by training so need precautions to avoid predictions you dont want in deployment\n",
    "#### BERT has been pretrained in filling masked words - change gender outputs completely different items - this MAN works at more likely to output (carpenter, doctor, mechanic), female more likely to output (maid, nurse, waitress, teacher, prostitute)\n",
    "#### gpd2 trained on internet reddit so more sexist, etc...\n",
    "#### these bias will persist even after fine-tuning - need enough samples of outputs want to see and always put model in production after analyzing results - if want to avoid some outputs try to provide more training data that correct the bias of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Transformers, Model APIs, and Tokenizers used to convert text into format models can process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens in pipeline function\n",
    "\n",
    "#### How sentiment pipeline went from positive to negative? converts raw text to numbers using a tokenizer, then those numbers go through model which outputs logits, then post-processing steps transform those delegates into labels and scores\n",
    "#### Tokenization process - text split into tokens, then tokenizer adds special tokens if it expects them to classify, then tokenizer matches each token to unique id in vocabulary of tokenizer - autotokeniszer api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2031,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I have been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "# Because two sentences passed in this model are not of the same size, need to pad the shortest one to be able to build an array using padding=True\n",
    "# Truncation=True is used to make sure that any sentence longer than the maximum the model can handle is truncated\n",
    "# return_tensors option tells the tokenizer to return the pytorch tensor\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# result is dictionary with 2 keys, input ids contains ids of both sentences with zero padding applied\n",
    "# the second, attention_mask, indicates that padding has been applied so the model does not pay attention to it\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# from_pretrained method will downlaod and cache configuration of model as well as the pre-trained weight. However, automodel api will on instantiate the body of the model (part of model left once pre-training head is removed)\n",
    "# outputs high dimensional tensor that is a representation of the sentences passed but which is not directly useful for classification problem\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# output torch.Size([2, 15, 768]) [batch length, sequence length, hidden size]\n",
    "# this output shows the tensor has 2 sentences, each of 16 tokens, and last timension is indent size of model 768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3782,  1.4346],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# to get output for classifcation problem need to use autoModelForSequenceClassification\n",
    "# works like automodel class but built a model with a classification head - there is one auto class for each common nlp task in transformers library\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)\n",
    "\n",
    "# output tensor([[-1.3782,  1.4346],\n",
    "#        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n",
    "\n",
    "# tensor size of 2x2, one result for each sentence and each possible label - these outputs are not probabilities yet - can see bc they dont sum to 1\n",
    "# this is because each model of the transformers library retuirns lockets - to make sens eof those logits need to look at post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Post-processing (logits to predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.6636e-02, 9.4336e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# to convert logits to probabilities need to apply soft max layers to them\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "\n",
    "# the output shows that these are positive numbers that sum to 1\n",
    "# last step is to know which of those correspond to the positive or negative level - given by id to label field of model config\n",
    "# first index [0] probabilities correspond to negative label, the second [1] correspond to positive label\n",
    "\n",
    "\n",
    "# output tensor([[5.6636e-02, 9.4336e-01],\n",
    "#        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this problem only has 2 classes, positive and negative, but what if you have a multi-class classification problem?\n",
    "# this one predicts two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.7393240332603455},\n",
       " {'label': 'POSITIVE', 'score': 0.8359946608543396}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline \n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\") \n",
    "classifier([ \n",
    "    \"There is a little truth behind every joke.\", \n",
    "    \"Time flies when you're having fun\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline first needs to process raw text using tokenizer and key thing to remember, if doing any fine-tuning, inference, or prediction, important that checkpoint use for tokenizer is same for model bc when transformers are pretrained on large corpus, there was correspoding tokenizer also trained to learn the vocabulary of that corpus - if mismatch you mismatch vocabulary and get inoptimal results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to instantiate the tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input ids are mapping to unique interger, its a mapping to vocabulary - each word has an integer mapped to it\n",
    "# attention mask puts 1s and 0s at end of sequence which we will discuss later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2045,  2003,  1037,  2210,  3606,  2369,  2296,  8257,  1012,\n",
      "           102],\n",
      "        [  101,  2051, 10029,  2043,  2017,  1005,  2128,  2383,  4569,   102,\n",
      "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [ \n",
    "    \"There is a little truth behind every joke.\", \n",
    "    \"Time flies when you're having fun\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now load model which processes inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"finetuning_task\": \"sst-2\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do this in the model config\n",
    "\n",
    "from transformers import AutoModel \n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "model.config\n",
    "\n",
    "#  every single model has a config and this config tells you things like the number of classes\n",
    "\n",
    "#  \"label2id\": {\n",
    "#    \"NEGATIVE\": 0,\n",
    "#    \"POSITIVE\": 1\n",
    "#  },\n",
    "\n",
    "# whjat you can do when you instantiate a model is define the number of classes you would like when you instantiate text classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example, say I have a checkpoint - jsut take distilbert-base-uncased model for example\n",
    "# then will import model for sequence classification - good for classification, multi-class\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint_for_multiclass = \"distilbert-base-uncased\"\n",
    "\n",
    "# then take model for sequence classification - from pretrained and take checkpoint and pass keyword arguments that specify how many labels we are dealing with\n",
    "# so downloads base pretained model and adds classification head ontyop of model and configure with right number of classes\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_for_multiclass, num_labels=6)\n",
    "\n",
    "model.config\n",
    "\n",
    "#  \"id2label\": {\n",
    "#     \"0\": \"LABEL_0\",\n",
    "#     \"1\": \"LABEL_1\",\n",
    "#     \"2\": \"LABEL_2\",\n",
    "#     \"3\": \"LABEL_3\",\n",
    "#     \"4\": \"LABEL_4\",\n",
    "#     \"5\": \"LABEL_5\"\n",
    "#   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simplest way to feed inputs to model is using python unpacking operator to feed all keys and values to the model\n",
    "# this feed all inputs to forward pass of model to genrate the outputs\n",
    "# the output is taking raw text, converting to numbers, and then converting those integers into dense vectors - so every token is associated witha  vector \n",
    "# torch.Size([2, 11, 768]) means 2 sentences, 11 vectors per sentence, and each vector has 768 dimensions bc of the way BERT was pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel \n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all outputs are wrapped in an object which is something we can index by attribute name - basemodeloutput, has last_hidden_state and tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.2046,  0.3315,  0.0793,  ...,  0.0537,  0.2866,  0.2205],\n",
       "         [-0.2585,  0.4289, -0.0989,  ..., -0.1086,  0.1441,  0.3349],\n",
       "         [-0.3724,  0.4050, -0.0340,  ..., -0.1534,  0.3746,  0.6864],\n",
       "         ...,\n",
       "         [-0.0311,  0.3458,  0.0469,  ..., -0.0710,  0.3344,  0.5246],\n",
       "         [ 0.5216,  0.1719,  0.1227,  ...,  0.3565, -0.1977, -0.1930],\n",
       "         [ 0.3645,  0.4160,  0.4990,  ...,  0.3746, -0.0586, -0.0142]],\n",
       "\n",
       "        [[ 0.2481,  0.2188,  0.6047,  ..., -0.7964,  0.9062, -0.0586],\n",
       "         [ 0.3293,  0.5613,  0.3619,  ..., -0.6319,  0.6820, -0.1266],\n",
       "         [ 0.4563,  0.5079,  0.5108,  ..., -0.7594,  0.3598,  0.1546],\n",
       "         ...,\n",
       "         [ 0.5942,  0.0251,  0.6978,  ...,  0.0855,  0.7381, -0.3256],\n",
       "         [ 0.9546,  0.4101,  0.8726,  ...,  0.4479,  0.5824, -0.6132],\n",
       "         [ 0.1438, -0.0730,  0.5039,  ..., -0.1315,  0.6125, -0.2272]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# so if want to look at last_hidden_state of output, first sentence, vector correspodning to first token\n",
    "# this huge lsit of numbers negative to positive should have a size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0456e-01,  3.3149e-01,  7.9314e-02,  2.3897e-01, -2.6303e-01,\n",
       "         5.2754e-02,  2.4966e-01,  3.7042e-01,  1.4424e-01, -5.9761e-01,\n",
       "         4.8752e-01,  4.8655e-02,  1.7460e-01,  1.1068e-01, -2.8549e-01,\n",
       "         5.5047e-01,  4.8756e-01, -5.9484e-02,  1.1203e-01, -3.6063e-01,\n",
       "        -1.0464e-01, -5.5221e-03, -1.3071e-01,  7.7812e-02,  2.4495e-01,\n",
       "         2.3912e-01,  8.9579e-03,  2.3336e-01, -1.9855e-01,  3.8994e-01,\n",
       "        -8.8543e-02, -7.9171e-03, -1.4176e-01, -3.8431e-01,  4.0755e-01,\n",
       "        -1.7427e-02, -3.0652e-01,  6.5352e-01, -4.8395e-02,  1.9115e-01,\n",
       "        -1.3361e-01,  6.9770e-01, -5.2213e-02, -1.6725e-01, -5.1267e-01,\n",
       "        -1.8223e-01, -1.1495e+00,  1.9498e-02,  2.5056e-01, -2.8728e-01,\n",
       "        -2.1412e-01,  3.2262e-01, -1.3824e-01,  4.6103e-01,  2.8472e-01,\n",
       "         7.8934e-01,  3.3711e-01, -2.9545e-01, -1.3582e-01,  1.2684e-01,\n",
       "        -6.2874e-02,  3.7624e-01,  3.3091e-01, -2.8465e-01,  9.3525e-02,\n",
       "         1.4451e+00, -2.4522e-01,  2.3340e-01, -6.4958e-01,  5.3894e-01,\n",
       "        -1.8922e-01, -1.1364e-01, -1.1028e-01, -4.3180e-01, -1.8560e-02,\n",
       "        -3.2386e-02, -9.8908e-02,  1.3738e-01,  4.3722e-01,  8.3683e-01,\n",
       "        -9.4027e-03, -4.8607e-01,  4.7615e-01, -1.8755e-01,  3.6544e-01,\n",
       "        -2.1969e-03,  7.3058e-01,  5.2322e-01, -4.2351e-01, -9.1186e-02,\n",
       "         4.7429e-01, -1.0427e-01, -4.0959e-02,  7.9973e-02, -3.9176e-01,\n",
       "        -2.1614e-01,  6.0827e-01, -2.8410e-01,  5.0215e-01,  1.0952e-01,\n",
       "         7.9034e-02, -2.3338e-01,  2.8822e-01,  2.4933e-03, -1.3106e-01,\n",
       "         4.6719e-01, -2.2772e-01, -1.2684e-01, -5.8281e-01,  4.4273e-01,\n",
       "        -3.2948e-01, -1.5931e-01, -1.7427e-01, -4.2760e-02, -8.0718e-01,\n",
       "         2.6571e-02, -3.8222e-01, -1.7780e-01,  9.2554e-01,  4.8143e-01,\n",
       "         1.5868e-01, -2.4917e-02,  2.1071e-01, -1.1530e-02,  1.7496e-01,\n",
       "         1.1396e-01, -5.8377e-01,  4.8083e-01, -5.7698e-01,  4.2729e-01,\n",
       "         5.7575e-01,  1.0047e+00, -6.1809e-02,  2.3078e-01,  3.9164e-02,\n",
       "        -3.2149e-02,  5.7678e-01, -8.3378e-02,  3.4886e-02, -5.1409e-01,\n",
       "        -4.1936e-01, -3.4375e-01, -2.4449e-01,  5.4935e-01, -1.3455e-01,\n",
       "        -1.5542e-01, -2.6220e-01, -1.2413e-01, -1.9230e-01,  7.7619e-02,\n",
       "         1.0295e-02, -7.9291e-02, -7.2798e-02, -5.4830e-01, -5.6349e-01,\n",
       "         7.7388e-03, -1.0936e+00, -2.2610e-01,  3.2419e-02,  3.8085e-01,\n",
       "        -3.6791e-01, -6.0735e-02,  1.6118e-01, -4.5892e-01,  3.5350e-01,\n",
       "        -4.9482e-01, -2.9604e-01, -5.1514e-01, -1.7173e-01,  3.8282e-01,\n",
       "        -6.5337e-01,  2.2566e-01,  1.0763e+00, -2.2731e-01,  3.4471e-01,\n",
       "         2.2104e-01, -3.6268e-01,  2.6430e-01,  2.9853e-01,  2.7447e-02,\n",
       "        -8.2494e-01,  1.6154e-01, -2.2091e-01, -3.4946e-01,  3.4172e-02,\n",
       "        -8.6974e-02,  1.8477e-01, -4.6655e-01, -3.2510e-01, -6.1668e-01,\n",
       "        -5.7932e-01, -4.0171e-01, -2.0814e-02, -6.8916e-02,  5.6469e-01,\n",
       "        -8.1945e-01, -6.9457e-02,  8.2401e-01,  3.5895e-01,  4.3797e-01,\n",
       "        -1.8790e-01, -2.2637e-01, -6.1307e-01,  4.0365e-01, -1.8941e-01,\n",
       "         1.4754e+00,  1.0656e-01,  3.4802e-02, -1.1368e-01,  5.2570e-01,\n",
       "        -3.4303e-01,  3.0840e-01, -4.8060e-01, -4.8903e-01,  1.9836e-01,\n",
       "         7.2243e-02,  6.4871e-01, -6.3289e-02,  8.0204e-01,  1.1213e-01,\n",
       "         1.6620e-01, -5.7279e-01,  6.4953e-02,  5.3355e-01, -1.2853e-01,\n",
       "         3.2726e-02,  5.0887e-01, -2.1612e-03,  3.6756e-01, -6.1028e-01,\n",
       "         2.3662e-02,  1.6027e-01,  7.6311e-01, -3.5175e-01,  3.8505e-01,\n",
       "        -2.5330e-01,  9.9221e-02,  5.1922e-02,  2.8311e-01,  1.0766e-01,\n",
       "        -3.3201e-01,  1.9974e-01, -1.7409e-01,  5.2855e-01,  8.0486e-02,\n",
       "         5.8051e-01, -2.2503e-01,  2.4167e-01, -4.9919e-01, -8.1221e-02,\n",
       "        -3.6834e-01,  5.1202e-02,  1.4033e-01, -4.1949e-01, -1.0900e-01,\n",
       "         1.1223e-01, -6.5397e-01, -5.9267e-01, -6.4807e-02,  7.2602e-01,\n",
       "        -6.9556e-01,  1.2624e-01, -4.3886e-01, -2.7108e-01, -3.4964e-01,\n",
       "         5.3005e-01,  1.4783e-03,  2.6496e-02,  4.2099e-01,  7.1102e-01,\n",
       "         1.8355e-01,  3.4314e-01, -2.7921e-01,  2.6833e-01,  3.6948e-01,\n",
       "        -3.8019e-02, -1.1074e-01,  4.8337e-01, -7.7527e-03, -3.5820e-01,\n",
       "         2.3794e-01,  1.0049e-01,  1.8118e-01,  1.5069e-01,  1.6053e-01,\n",
       "        -4.6872e-02,  5.6059e-01, -2.5385e-01, -6.0182e-01, -6.2994e-02,\n",
       "        -4.0355e-01,  5.0881e-01, -1.1724e-01, -2.0144e-01,  3.8852e-01,\n",
       "         4.5785e-02,  2.5673e-01, -1.2736e-01,  2.3900e-01, -1.1397e-02,\n",
       "         4.7227e-01,  3.0380e-01,  2.8562e-01,  8.1217e-02,  5.1566e-02,\n",
       "         2.0857e-01,  6.5952e-02, -1.9317e-01, -4.1993e+00,  4.2466e-02,\n",
       "        -3.0576e-01, -8.0315e-01,  9.0778e-02, -7.8105e-02, -4.5596e-01,\n",
       "        -2.6008e-01, -4.8939e-01, -6.0126e-01,  7.9440e-02,  2.8313e-01,\n",
       "         1.0340e-01, -3.7138e-02,  5.6893e-01, -3.7387e-01,  1.3942e-02,\n",
       "        -8.3705e-02, -1.3806e-01,  2.1310e-01, -6.8621e-02,  4.1534e-01,\n",
       "        -3.2442e-01, -1.7732e-01,  2.1967e-01,  6.6203e-01, -4.0945e-01,\n",
       "        -1.9309e-02, -6.6416e-01, -4.8050e-01, -4.3030e-03, -7.0739e-01,\n",
       "        -2.8653e-01, -7.0398e-01, -2.7786e-01, -4.1563e-01, -4.3694e-01,\n",
       "        -4.5030e-01,  8.5329e-02, -7.1331e-02, -3.9555e-01, -3.0058e-01,\n",
       "         4.1859e-01,  2.9014e-01,  5.3905e-01, -1.3454e-01,  2.4685e-01,\n",
       "         1.2908e-01,  4.3429e-02,  4.6517e-03,  1.0557e-01,  3.5365e-01,\n",
       "        -5.6928e-02, -2.0041e-01, -3.2843e-01, -9.9456e-02,  2.7072e-01,\n",
       "        -1.5238e-01, -3.1542e-01, -1.1375e-01, -2.7941e-01, -9.4819e-02,\n",
       "        -1.7043e-01, -2.0947e-01,  3.4264e-01,  5.0121e-02,  4.0387e-01,\n",
       "        -1.7951e-02,  3.5505e-01,  1.9675e-01, -1.0554e-01, -1.2906e-01,\n",
       "         1.6273e-01, -5.4193e-01,  1.1945e+00,  3.7285e-01, -5.2380e-01,\n",
       "         2.7671e-02,  2.8418e-01, -2.1957e-01, -2.5226e-01, -3.0333e-01,\n",
       "        -1.2442e-01,  1.4994e-01, -3.0665e-01, -5.4868e-01, -2.2697e-01,\n",
       "        -5.9201e-02, -7.7487e-01, -2.1508e-01,  7.3893e-01, -1.1186e-01,\n",
       "         2.2652e-01,  7.8416e-01,  6.7122e-01,  5.8587e-01,  9.9345e-02,\n",
       "         1.1252e-02,  4.1457e-02, -1.6880e-01,  4.8399e-01, -1.4771e-01,\n",
       "         3.3534e-01,  3.6102e-02, -6.0753e-01, -4.6671e-01, -3.2333e-01,\n",
       "         8.3527e-01,  4.9032e-01, -2.2425e-01,  2.4375e-01,  3.6399e-01,\n",
       "         7.0466e-01,  6.1255e-01,  4.6871e-02, -2.2962e-01, -1.4674e-02,\n",
       "         2.6958e-01, -4.8237e-01, -5.5014e-02, -1.0813e-01,  2.7208e-01,\n",
       "         2.6545e-01,  1.8244e-01, -8.1046e-01,  1.0396e-01,  2.4040e-01,\n",
       "         3.6506e-02, -3.6751e-03,  3.8655e-02, -1.5241e-01,  6.1852e-01,\n",
       "         2.2841e-01, -2.5350e-01, -2.7668e-01, -3.4840e-02,  7.2541e-02,\n",
       "         6.6428e-02,  4.7436e-01,  7.1286e-02,  2.5543e-01,  3.8631e-01,\n",
       "         4.9953e-01,  1.9249e-01, -7.0621e-01,  2.0347e-01, -1.5598e-01,\n",
       "        -1.3301e-01, -1.8968e-01, -7.5328e-02, -1.6167e-01, -1.9272e-01,\n",
       "         4.8692e-01, -1.1703e-01,  3.8724e-01, -2.1711e-01,  6.7814e-03,\n",
       "         3.7560e-02, -5.1271e-01,  9.5108e-02,  2.1842e-01,  5.8243e-02,\n",
       "        -5.3080e-01,  2.0677e-01,  5.1179e-01,  1.8366e-01, -8.5380e-01,\n",
       "        -3.9678e-02, -2.4113e-03,  5.0145e-01, -1.7881e-02, -3.0244e-02,\n",
       "         1.6801e-01,  2.3860e-01,  9.9747e-02,  3.0000e-01, -2.1199e-01,\n",
       "        -2.8028e-01, -6.0742e-01, -2.9266e-01,  2.1023e-02, -3.1946e-01,\n",
       "        -2.3992e-01, -3.0335e-01,  4.9134e-01, -3.3741e-01, -1.3665e-01,\n",
       "         1.8056e-01, -1.8137e-01, -1.0966e-01,  2.1482e-01, -3.9689e-02,\n",
       "         1.8390e-01,  2.6993e-01,  2.1574e-01, -3.5769e-01, -1.3750e-01,\n",
       "        -1.5304e-01,  1.7268e-01, -4.9063e-01, -2.0329e-01, -4.4588e-01,\n",
       "        -4.0373e-01, -2.8289e-01,  2.6293e-01, -4.9057e-02,  4.1153e-01,\n",
       "         1.9075e-01, -2.8889e-01,  6.7639e-01, -7.4189e-01, -7.5923e-01,\n",
       "        -5.2784e-01,  5.0188e-01,  6.1354e-01, -5.3092e-01, -1.9160e-01,\n",
       "         3.2524e-01,  5.9706e-02, -6.3270e-01,  4.2391e-01, -1.0769e+00,\n",
       "        -1.1921e-01, -8.7553e-02, -4.3153e-02,  4.8946e-01, -3.5953e-01,\n",
       "        -6.3994e-01,  6.3159e-01, -4.3562e-01,  7.8208e-02, -2.4832e-01,\n",
       "         6.6367e-01,  1.3047e-01, -7.4433e-01,  1.3235e-01, -1.3227e-01,\n",
       "         8.8532e-01, -1.6101e-01, -4.2655e-01, -8.9274e-02,  9.2020e-02,\n",
       "        -2.3634e-01, -2.5547e-01, -2.9059e-01,  4.1640e-02,  1.8836e-01,\n",
       "        -4.0628e-02, -1.0129e+00,  2.2338e-01,  2.7011e-01, -4.1737e-01,\n",
       "        -1.4337e-01, -2.3134e-01, -1.6145e-01,  3.3997e-01, -3.5596e-01,\n",
       "        -1.8588e-01,  1.5797e-02, -4.1616e-01,  2.8649e-01, -5.6133e-01,\n",
       "        -1.6114e-02, -3.8632e-01, -8.7841e-02, -3.6530e-01, -6.5956e-02,\n",
       "        -5.8822e-02,  2.7294e-01,  1.4804e-01, -2.2409e-01, -5.8941e-01,\n",
       "         3.5796e-01, -1.9110e-02, -3.7403e-01,  9.8852e-01, -8.9389e-01,\n",
       "         1.3725e-01,  5.7069e-02, -2.7104e-01,  1.4138e-01, -8.1888e-02,\n",
       "         9.8694e-02, -2.4284e-01,  8.8006e-02,  3.6698e-01,  1.9921e-01,\n",
       "         2.1457e-01,  2.3304e-01, -2.4784e-01, -5.8306e-01,  1.3248e-01,\n",
       "         2.8311e-01, -3.6459e-01, -2.6379e-01, -6.4041e-02,  2.4170e-01,\n",
       "        -2.0576e-03,  8.6367e-01, -2.4834e-01, -6.6098e-02,  4.9962e-01,\n",
       "         8.0575e-01, -7.7928e-01,  4.2093e-01,  3.4535e-02, -2.1729e-01,\n",
       "        -3.9671e-01,  3.4014e-01, -4.9943e-03,  2.4785e-01,  9.0179e-02,\n",
       "         2.0219e-01, -6.2084e-02,  5.0717e-02, -4.4458e-01,  5.3895e-01,\n",
       "        -6.6504e-01, -1.7722e-01,  3.8306e-01,  2.0935e-01,  6.7339e-02,\n",
       "         3.7038e-01, -6.3570e-02,  3.5917e-01,  3.9490e-01, -2.6851e-01,\n",
       "         4.0800e-02,  1.4439e-01,  3.4929e-01,  2.7578e-02,  3.6896e-01,\n",
       "        -5.4325e-01, -3.7726e-01, -3.5511e-01, -3.2394e-01,  4.6791e-01,\n",
       "        -2.3101e-01,  8.9164e-01,  1.1135e-01, -7.0829e-02, -8.3628e-01,\n",
       "        -2.8462e-01,  3.1853e-01, -7.7520e-02,  4.3054e-01,  6.7048e-01,\n",
       "        -2.2046e-01, -2.0375e-01,  4.0007e-01, -1.3297e-01,  1.0889e-03,\n",
       "        -2.0778e-01, -2.6795e-01,  1.1014e-01, -1.7480e-04, -1.2885e-01,\n",
       "         2.5599e-01, -6.5352e-01,  1.0204e+00,  3.3118e-03, -5.7649e-01,\n",
       "         6.2996e-01,  2.2719e-01,  8.8079e-02,  7.2543e-01,  7.1761e-01,\n",
       "        -5.0199e-01,  3.7996e-01, -2.8679e-01, -2.9468e-01, -2.8520e-01,\n",
       "        -8.7176e-02,  3.5091e-01, -2.2402e-01,  8.6220e-03,  4.0058e-01,\n",
       "         7.0127e-01, -5.0751e-01, -3.8007e-01,  4.3014e-01,  6.3042e-01,\n",
       "        -1.1918e+00, -1.0740e-01,  7.0941e-01,  5.4826e-02,  1.8437e-01,\n",
       "         2.4701e-01, -3.7376e-01, -7.3031e-02, -5.9721e-01, -2.9527e-03,\n",
       "        -1.3599e-01, -2.6550e-02,  1.3769e-01, -3.7830e-01,  9.7123e-02,\n",
       "        -3.6157e-01,  3.9351e-02,  4.8248e-02,  4.0227e-02,  7.5673e-01,\n",
       "         1.6475e-01,  4.4543e-01, -7.5364e-01,  5.5685e-01, -1.7705e-01,\n",
       "         2.3157e-01, -1.0464e+00, -1.9496e-01,  2.1049e-01,  1.6479e-01,\n",
       "        -8.5150e-01, -7.6640e-02, -5.8322e-01, -1.8347e-01,  3.3681e-01,\n",
       "        -2.4624e-01,  1.7806e-01, -3.0621e-01, -5.0245e-01, -4.1269e-02,\n",
       "         5.5126e-02,  1.7006e-01,  9.0580e-02, -2.0112e-01,  3.6183e-01,\n",
       "         1.2392e-01, -8.4109e-01,  1.7533e-01, -4.8772e-01, -2.6703e-01,\n",
       "         5.9003e-01, -5.5179e-01,  2.9966e-01, -4.2953e-01,  4.4297e-01,\n",
       "         1.8476e-01, -2.3815e-01, -3.8363e-01,  1.9120e-01, -3.8810e-01,\n",
       "        -6.2767e-01,  4.6572e-02, -2.6106e-01,  7.7355e-02, -3.3773e-01,\n",
       "        -5.1155e-01, -3.2185e-01, -4.3002e-03, -4.6529e-02, -1.7187e-01,\n",
       "        -4.5836e-01, -5.2004e-01,  3.0722e-01, -1.7559e-01,  4.2462e-01,\n",
       "         5.3671e-02,  2.8660e-01,  2.2047e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numerical representations by themselves dont let us do things like text classification - they just let us say numerical classification is x\n",
    "# if want to do classificaiton need to add vectors with classification head - entire transformer library is built around idea of taking a model for task x like summarization, classification, question/answering\n",
    "# when substantiate model with sequence classification - has model with number of labels based on checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6482, -0.3943],\n",
       "        [-0.7400,  0.8887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# when look at outputs - instead of just last hidden state - you have logits - these are compressed 768 dimensional vectors into numbers used to provide probabilities and indicates which class is most likely\n",
    "# in this example positive or negative sentiment condenses into 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# so first sentence is more likely to be positive, second is more likely to be negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6482, -0.3943],\n",
      "        [-0.7400,  0.8887]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if want to convert logits to probabiltiies, take a softmax over them - which takes all of the inputs, exponentiates them and normalized that by the sum of the exponentials (ranges from 0-1) - good for probabilitiess - provides probabilties for both sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7393, 0.2607],\n",
      "        [0.1640, 0.8360]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can see the labeling for each of them in human-readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using decode method can see raw text of token ids\n",
    "# include special tokens like [SEP] which shows sepeeration between pairs of tokens or [CLS] token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] there is a little truth behind every joke. [SEP]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Instantiate a Transformer Model from Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartModel'>\n"
     ]
    }
   ],
   "source": [
    "# use automodel and from pretrained selects model inside\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model =AutoModel.from_pretrained(\"gpt2\")\n",
    "print(type(gpt_model))\n",
    "\n",
    "bart_model = AutoModel.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can use autoconfig class to easily load the configuration of a pretrained model from any checkpoint to pick the right configuration class from the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n"
     ]
    }
   ],
   "source": [
    "# can use specific class corresponding to a checkpoint but need to change the code each time you want to try a different model architecture\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.configuration_bart.BartConfig'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartConfig \n",
    "\n",
    "bart_config = BartConfig.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config of model is a blueprint that contains all the info necessary to create model architecture\n",
    "# model was built with 12 layers hidden, size of 768, and vocab sidze oof 28,996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig \n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(bert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# once have configuration, can create model that has same architecture as checkpoint but is randomly initialized and then train from scratch like any pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses same architecture as bert-base-cased\n",
    "\n",
    "from transformers import BertConfig, BertModel \n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "bert_model = BertModel(bert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can also change any part of the configuration by using key word arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only 10 layers instead of 12\n",
    "\n",
    "from transformers import BertConfig, BertModel \n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\", num_hidden_layers=10)\n",
    "bert_model = BertModel(bert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving a model once it's trained or fine-tuned using the save_pretrained method which saved in cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel \n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "bert_model = BertModel(bert_config)\n",
    "\n",
    "# training code\n",
    "\n",
    "bert_model.save_pretrained(\"my-bert-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reloading a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel \n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"my-bert-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# once you have saved your model you have config file and  pytorch_model.bin - the later is a state dictionary that has all weights - if want to use it take input text, convert to input ids, convert input ids to tensors which feed the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in practice most of the time using from_pretrained as it will initialize model with pretrained weights and correct head if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model so can deploy it somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7592,  999,  102],\n",
       "        [ 101, 4658, 1012,  102],\n",
       "        [ 101, 3835,  999,  102]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is what constitues a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.4496e-01,  4.8276e-01,  2.7797e-01,  ..., -5.4032e-02,\n",
       "           3.9394e-01, -9.4770e-02],\n",
       "         [ 2.4943e-01, -4.4093e-01,  8.1772e-01,  ..., -3.1917e-01,\n",
       "           2.2992e-01, -4.1172e-02],\n",
       "         [ 1.3668e-01,  2.2518e-01,  1.4502e-01,  ..., -4.6914e-02,\n",
       "           2.8224e-01,  7.5566e-02],\n",
       "         [ 1.1789e+00,  1.6738e-01, -1.8187e-01,  ...,  2.4671e-01,\n",
       "           1.0441e+00, -6.1961e-03]],\n",
       "\n",
       "        [[ 3.6436e-01,  3.2464e-02,  2.0258e-01,  ...,  6.0111e-02,\n",
       "           3.2451e-01, -2.0996e-02],\n",
       "         [ 7.1866e-01, -4.8725e-01,  5.1740e-01,  ..., -4.4012e-01,\n",
       "           1.4553e-01, -3.7545e-02],\n",
       "         [ 3.3223e-01, -2.3271e-01,  9.4876e-02,  ..., -2.5268e-01,\n",
       "           3.2172e-01,  8.1111e-04],\n",
       "         [ 1.2523e+00,  3.5754e-01, -5.1320e-02,  ..., -3.7840e-01,\n",
       "           1.0526e+00, -5.6255e-01]],\n",
       "\n",
       "        [[ 2.4042e-01,  1.4718e-01,  1.2110e-01,  ...,  7.6062e-02,\n",
       "           3.3564e-01,  2.8262e-01],\n",
       "         [ 6.5701e-01, -3.2787e-01,  2.4968e-01,  ..., -2.5920e-01,\n",
       "           2.0175e-01,  3.3275e-01],\n",
       "         [ 2.0160e-01,  1.5783e-01,  9.8971e-03,  ..., -3.8850e-01,\n",
       "           4.1308e-01,  3.9732e-01],\n",
       "         [ 1.0175e+00,  6.4387e-01, -7.8147e-01,  ..., -4.2109e-01,\n",
       "           1.0925e+00, -4.8455e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6856,  0.5262,  1.0000,  ...,  1.0000, -0.6112,  0.9971],\n",
       "        [-0.6055,  0.4997,  0.9998,  ...,  0.9999, -0.6753,  0.9769],\n",
       "        [-0.7702,  0.5447,  0.9999,  ...,  1.0000, -0.4655,  0.9894]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### power of transformers is bc we dont have to pretrain - expesive and time consuming\n",
    "#### only time might really be stuck is if dealing with domain that is different from any existing pretrained model like source code back in the day - understanding python when using english as the base wouldnt give results - would want to train on source code corpus\n",
    "#### generally also need to find alternative when dealing with language that isnt commonly supported (e.g. many langugages in africa arent highly supported on wikipedia) - adapt something multilingual like BERT and use it on another language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers in more detail (3 most popular approaches)\n",
    "## most common tokenizers are wordpiece (bert) or sentencepiece (gpt models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split using whitespace\n",
    "#### split text into words on whitespace in english as this is the boundary between words - so a word is a token (but not japanese!)\n",
    "#### need a token for every word in language - english several hundred thousand tokens which is expensive and no distinction between dog and dogs - similar but now representing them with two tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to use autotokenizer since automatically uses based on checkpoint provided\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## differentiates words from subwords using ## symbol (double hash) / can tell its good to split trans from everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconstruct sentences by converting back to tokens/input ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this approach provides special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sequence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Using a Transformer network is simple [SEP]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unless you skip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using a Transformer network is simple'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different tokenizer - gpt is quirky using symbol ontop of letters to indicate there is whitespace between two tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'Ġa', 'ĠTrans', 'former', 'Ġnetwork', 'Ġis', 'Ġsimple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character based approach\n",
    "#### split text into characters rather than words - benefits are vocabulary is much smaller and fewer out of vocabulary tokens since every word can be built from characters\n",
    "#### model has to learn what a word actually means - only gets characters and then says if i put these characters in this order represents more abstract object like a word not good for english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization\n",
    "#### instead of splitting on word boundaries or characters, decompose a word into subwords (e.g. annoyingly \"annoying\" \"ly\" and collect frequencies of subwords and use this to figure out what are most frequent subwords in language and use that to build back the word itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling multiple sequences together\n",
    "## how to batch inputs together? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 1842, 11065, 379, 9309, 2059, 13]\n",
      "[40, 5465, 11065, 379, 9309, 2059, 3360, 13]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentences = [ \n",
    "    \"I love studying at Columbia University.\", \n",
    "    \"I hate studying at Columbia University sometimes.\",\n",
    "]\n",
    "\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "print(ids[0])\n",
    "print(ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this throws an error bc both sentences need to be same length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 7 at dim 1 (got 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17808/4201695064.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#this will fail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 7 at dim 1 (got 8)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ids =[[40, 1842, 11065, 379, 9309, 2059, 13],[40, 5465, 11065, 379, 9309, 2059, 3360, 13]]\n",
    "\n",
    "#this will fail\n",
    "input_ids = torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## can overcome this by passing 0s so they are same length as many times as necessary or truncate the length of the longer sequence to length of shorter sequence but then lose a lot of information\n",
    "## in general, we only truncate sentences when they are longer than the maximum length the model can handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ids =[[40, 1842, 11065, 379, 9309, 2059, 13, 0],[40, 5465, 11065, 379, 9309, 2059, 3360, 13]]\n",
    "input_ids = torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the value used to pad the sentence should not be picked randomly - the model has been pretrained with padding id which you can find in tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dont get same result for sentence that is padded, attention layers just the tokens in the unpadded sentence, attention layers attend the tokens and all padded tokens in the sentence with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3128, -2.0399]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.7803, -2.3437]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.3128, -2.0399],\n",
      "        [ 2.7803, -2.3437]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "ids1 = torch.tensor([[40, 1842, 11065, 379, 9309, 2059, 13, 0]])\n",
    "ids2 = torch.tensor([[40, 5465, 11065, 379, 9309, 2059, 3360, 13]])\n",
    "all_ids = torch.tensor([[40, 1842, 11065, 379, 9309, 2059, 13, 0],[40, 5465, 11065, 379, 9309, 2059, 3360, 13]])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "print(model(ids1).logits)\n",
    "print(model(ids2).logits)\n",
    "print(model(all_ids).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3128, -2.0399]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.7803, -2.3437]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "output1  = model(ids1)\n",
    "output2 = model(ids2)\n",
    "print(output1.logits)\n",
    "print(output2.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention mask 1 indicate should consider it in context and 0 are tokens we should ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = torch.tensor([[40, 1842, 11065, 379, 9309, 2059, 13, 0],[40, 5465, 11065, 379, 9309, 2059, 3360, 13]])\n",
    "attention_mask = torch.tensor(\n",
    "    [[1, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now passing with attention mask will provide same output for a given sentence without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2226, -1.8951],\n",
      "        [ 2.7803, -2.3437]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(all_ids, attention_mask=attention_mask)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is all done behind the scenes but the autotokenizer when you pass padding=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify padding technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sequence, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences =[\"my dog is called fido\", \"my cat is called something really cool like Michael\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2026, 3899, 2003, 2170, 10882, 3527, 102, 0, 0, 0], [101, 2026, 4937, 2003, 2170, 2242, 2428, 4658, 2066, 2745, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sequences, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this will process 512 tokens which is max length of bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2026, 3899, 2003, 2170, 10882, 3527, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2026, 4937, 2003, 2170, 2242, 2428, 4658, 2066, 2745, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sequences, padding=\"max_length\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2026, 3899, 2003, 2170, 10882, 3527, 102, 0, 0, 0], [101, 2026, 4937, 2003, 2170, 2242, 2428, 4658, 2066, 2745, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sequences, padding=\"longest\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2026, 3899, 2003, 2170, 10882, 3527, 102, 0, 0, 0], [101, 2026, 4937, 2003, 2170, 2242, 2428, 4658, 2066, 2745, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.padding_side = \"left\"\n",
    "\n",
    "tokenizer(sequences, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together - create own custom pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## truncation - long sequences attention is hard to do and most models predefine maximum length in pretraining phase - so truncate often when using pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sequence, return_tensors=\"pt\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now converted inputs down to maximum size model allows - do when text is too long\n",
    "### good for classification bc most stuff as at begining, qa is bad bc answer is at end of text \n",
    "### summarization depends on the case - sometimes need to do clever things like split text into different pieces, truncate those pieces, then aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Pytorch\n",
    "#### how to prepare large dataset from hub - github.com/huggingface/datasets\n",
    "#### train first model using trainer API to fine-tune a model\n",
    "#### how to use a custom training loop\n",
    "#### how to leverage the accelerate library to easily run that custom training loop on any distributed setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\creeg\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 919.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# mrpc dataset contains pairs of sentences for paraphrasing, glue benchmark\n",
    "#returns dataset dict - eachs plit indexed by its name, with columns sentence1 label idx and number of rows\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slice of dataset - list of second sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       "  \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
       "  'They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',\n",
       "  'Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n",
       "  'The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .'],\n",
       " 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       "  \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\n",
       "  \"On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\",\n",
       "  'Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',\n",
       "  'PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'],\n",
       " 'label': [1, 0, 1, 0, 1],\n",
       " 'idx': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw_datasets[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features gives us more information about columns - the correspondence between integers and names for labels, 0 means not equivalent and 1 means equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to process elements in our dataset, we need to tokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\creeg\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-62a09ac0c99ae1b1.arrow\n",
      "100%|██████████| 408/408 [00:00<00:00, 3923.03ex/s]\n",
      "Loading cached processed dataset at C:\\Users\\creeg\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-d715dbfd8732afb3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], 'validation': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], 'test': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# send sentences to tokenize with a few key word arguments\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"], example[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function)\n",
    "print(tokenized_datasets.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to speed up processing and to take advantage that tokenizer is backed by rust, can process several elements of same time using the batch=True argument\n",
    "### can also use map processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 19.28ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.03ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 19.61ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### can remove columns we dont need anymore with the remove columns method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
    "\n",
    "# rename label to labels\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "#can set back to desired format, torch/tensorflow/numpy\n",
    "tokenized_datsets = tokenized_datasets.with_format(\"torch\")\n",
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### can generate a short sample of a dataset using the select method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].select(range(100))\n",
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before - text classification head were adding ontop of model\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## dataset dict is a dictionary where the keys are a string that correspond to teh split, and values are dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\creeg\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 993.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label 1 indicates second sentence is actually a paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gives column name and data type of that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "}), Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 408\n",
       "}), Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 1725\n",
       "})])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tells me that the second sentence is a paraphrase of the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'equivalent'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features[\"label\"].int2str(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the first is not a paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not_equivalent'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features[\"label\"].int2str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n', citation='@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n', homepage='https://www.microsoft.com/en-us/download/details.aspx?id=52398', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='glue', config_name='mrpc', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=943851, num_examples=3668, dataset_name='glue'), 'validation': SplitInfo(name='validation', num_bytes=105887, num_examples=408, dataset_name='glue'), 'test': SplitInfo(name='test', num_bytes=442418, num_examples=1725, dataset_name='glue')}, download_checksums={'https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv': {'num_bytes': 6222, 'checksum': '971d7767d81b997fd9060ade0ec23c4fc31cbb226a55d1bd4a1bac474eb81dc7'}, 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt': {'num_bytes': 1047044, 'checksum': '60a9b09084528f0673eedee2b69cb941920f0b8cd0eeccefc464a98768457f89'}, 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt': {'num_bytes': 441275, 'checksum': 'a04e271090879aaba6423d65b94950c089298587d9c084bf9cd7439bd785f784'}}, download_size=1494541, post_processing_size=None, dataset_size=1492156, size_in_bytes=2986697)\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_dataset.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for private data if you cant use hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data locally - and use datasets functionaity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello world!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g day!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text  label\n",
       "0  Hello world!      1\n",
       "1        g day!      1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"text\": [\"Hello world!\", \"g day!\"], \"label\": [1, 1]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and create your own dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dset = Dataset.from_pandas(df)\n",
    "dset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and create your own dataset object that can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Hello world!', 'g day!'], 'label': [1, 1]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize sentence columns into input ids corresponding to first and second sentences c- these go into embedding layer turns into logits that go into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 26.4kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 792kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 3.68MB/s]\n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 5.79MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is how you can tokenize an entire dataset - just define a function to operate on every row of dataset and define whatever the operation you define as the function - then define a map onto the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applies function onto everything in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 23.52ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 43.46ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 26.14ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple example of adding column using a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column(row):\n",
    "    # has to return a dictionary where key is name of column and key is value you want in that row\n",
    "    return {\"new_column\": \"hello!\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when apply map, feed column that is added to new dataset\n",
    "### this operation IS NOT IN PLACE\n",
    "### if you want that column storeed in memeory you need to define a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\creeg\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-36e4d6bb26ef4768.arrow\n",
      "Loading cached processed dataset at C:\\Users\\creeg\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-36e4d6bb26ef4768.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0,\n",
       " 'new_column': 'hello!'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw_train_dataset.map(add_column)\n",
    "dataset_with_column = raw_train_dataset.map(add_column)\n",
    "\n",
    "dataset_with_column[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize raw datasets and now have tokenized datasets object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 17.79ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.86ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.68ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention mask tells you not to pay attention to padding - the 0s, the 1s we want the model to pay attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'idx': 0,\n",
       " 'input_ids': [101,\n",
       "  2572,\n",
       "  3217,\n",
       "  5831,\n",
       "  5496,\n",
       "  2010,\n",
       "  2567,\n",
       "  1010,\n",
       "  3183,\n",
       "  2002,\n",
       "  2170,\n",
       "  1000,\n",
       "  1996,\n",
       "  7409,\n",
       "  1000,\n",
       "  1010,\n",
       "  1997,\n",
       "  9969,\n",
       "  4487,\n",
       "  23809,\n",
       "  3436,\n",
       "  2010,\n",
       "  3350,\n",
       "  1012,\n",
       "  102,\n",
       "  7727,\n",
       "  2000,\n",
       "  2032,\n",
       "  2004,\n",
       "  2069,\n",
       "  1000,\n",
       "  1996,\n",
       "  7409,\n",
       "  1000,\n",
       "  1010,\n",
       "  2572,\n",
       "  3217,\n",
       "  5831,\n",
       "  5496,\n",
       "  2010,\n",
       "  2567,\n",
       "  1997,\n",
       "  9969,\n",
       "  4487,\n",
       "  23809,\n",
       "  3436,\n",
       "  2010,\n",
       "  3350,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'label': 1,\n",
       " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding \n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': torch.Size([8, 67]),\n",
       " 'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More coming shortly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "... A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "... Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "... In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "... Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "... 2010 marriage license application, according to court documents.\n",
    "... Prosecutors said the marriages were part of an immigration scam.\n",
    "... On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "... After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "... Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "... All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "... Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "... Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "... The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "... Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "... Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "... If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "... \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> liana barrientos, 39, pleaded not guilty to two counts of \"offering a false instrument for filing in the first degree\" the marriages were part of an immigration scam, prosecutors say.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "inputs = tokenizer(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(\n",
    "     inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    " )\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0eebd8ec8ee1e3b0ea056353a4df6208bb651af5156ee9734311d083be2b2cbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
